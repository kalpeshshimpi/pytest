{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af124cd1-d5bc-4263-9bf3-5d06d3d290ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d56685da-70d8-407e-8bd9-f0724e46e423",
     "showTitle": false,
     "title": "--i18n-3fbfc7bd-6ef2-4fea-b8a2-7f949cd84044"
    }
   },
   "source": [
    "# Aggregation\n",
    "\n",
    "##### Objectives\n",
    "1. Group data by specified columns\n",
    "1. Apply grouped data methods to aggregate data\n",
    "1. Apply built-in functions to aggregate data\n",
    "\n",
    "##### Methods\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html\" target=\"_blank\">DataFrame</a>: **`groupBy`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html\" target=\"_blank\" target=\"_blank\">Grouped Data</a>: **`agg`**, **`avg`**, **`count`**, **`max`**, **`sum`**\n",
    "- <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\" target=\"_blank\">Built-In Functions</a>: **`approx_count_distinct`**, **`avg`**, **`sum`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78427b9a-33ea-4762-b314-95d80964ea1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-00.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "036feb58-5c18-4cf8-ae14-90cf0cd14017",
     "showTitle": false,
     "title": "--i18n-88095892-40a1-46dd-a809-19186953d968"
    }
   },
   "source": [
    "\n",
    "Let's use the BedBricks events dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1521e69f-e271-4f71-958b-f7701a2bc7a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.table(\"events\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a717d70-3f47-4699-be2c-7bc96883a7e8",
     "showTitle": false,
     "title": "--i18n-a04aa8bd-35f0-43df-b137-6e34aebcded1"
    }
   },
   "source": [
    "\n",
    "### Grouping data\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/aspwd/aggregation_groupby.png\" width=\"60%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b5b1491-93d3-492b-9bd1-76d4ac97bf05",
     "showTitle": false,
     "title": "--i18n-cd0936f7-cd8a-4277-bbaf-d3a6ca2c29ec"
    }
   },
   "source": [
    "\n",
    "### groupBy\n",
    "Use the DataFrame **`groupBy`** method to create a grouped data object. \n",
    "\n",
    "This grouped data object is called **`RelationalGroupedDataset`** in Scala and **`GroupedData`** in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1faef1fd-ee77-4b7c-af87-2cf49260820f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"event_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d2e97b6-082f-4bd5-ab81-ce51c046ea5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.groupBy(\"geo.state\", \"geo.city\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c0b53d-4d41-4ca8-a20c-2929b9c97224",
     "showTitle": false,
     "title": "--i18n-7918f032-d001-4e38-bd75-51eb68c41ffa"
    }
   },
   "source": [
    "\n",
    "### Grouped data methods\n",
    "Various aggregation methods are available on the <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/grouping.html\" target=\"_blank\">GroupedData</a> object.\n",
    "\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| agg | Compute aggregates by specifying a series of aggregate columns |\n",
    "| avg | Compute the mean value for each numeric columns for each group |\n",
    "| count | Count the number of rows for each group |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| min | Compute the min value for each numeric column for each group |\n",
    "| pivot | Pivots a column of the current DataFrame and performs the specified aggregation |\n",
    "| sum | Compute the sum for each numeric columns for each group |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690ce8c7-703a-45ed-9e89-f6566e4ee082",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "event_counts_df = df.groupBy(\"event_name\").count()\n",
    "display(event_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddde656c-e0f7-4c22-b034-4ab823356af5",
     "showTitle": false,
     "title": "--i18n-bf63efea-c4f7-4ff9-9d42-4de245617d97"
    }
   },
   "source": [
    "\n",
    "Here, we're getting the average purchase revenue for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3a119c9-4fcc-45d9-8d17-86b60cb0d9b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "avg_state_purchases_df = df.groupBy(\"geo.state\").avg(\"ecommerce.purchase_revenue_in_usd\")\n",
    "display(avg_state_purchases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb7aef8-f36b-4709-bcb9-01a04e4f3c69",
     "showTitle": false,
     "title": "--i18n-b11167f4-c270-4f7b-b967-75538237c915"
    }
   },
   "source": [
    "And here the total quantity and sum of the purchase revenue for each combination of state and city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8b4132f-da67-4953-a372-de35726b0da2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "city_purchase_quantities_df = df.groupBy(\"geo.state\", \"geo.city\").sum(\"ecommerce.total_item_quantity\", \"ecommerce.purchase_revenue_in_usd\")\n",
    "display(city_purchase_quantities_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a5f43b4-be7a-44d1-a4ee-29e238488185",
     "showTitle": false,
     "title": "--i18n-62a4e852-249a-4dbf-b47a-e85a64cbc258"
    }
   },
   "source": [
    "\n",
    "## Built-In Functions\n",
    "In addition to DataFrame and Column transformation methods, there are a ton of helpful functions in Spark's built-in <a href=\"https://docs.databricks.com/spark/latest/spark-sql/language-manual/sql-ref-functions-builtin.html\" target=\"_blank\">SQL functions</a> module.\n",
    "\n",
    "In Scala, this is <a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/functions$.html\" target=\"_blank\">**`org.apache.spark.sql.functions`**</a>, and <a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions\" target=\"_blank\">**`pyspark.sql.functions`**</a> in Python. Functions from this module must be imported into your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7f988be-d2cc-4ee2-91e1-661db206e324",
     "showTitle": false,
     "title": "--i18n-68f06736-e457-4893-8c0d-be83c818bd91"
    }
   },
   "source": [
    "\n",
    "### Aggregate Functions\n",
    "\n",
    "Here are some of the built-in functions available for aggregation.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| approx_count_distinct | Returns the approximate number of distinct items in a group |\n",
    "| avg | Returns the average of the values in a group |\n",
    "| collect_list | Returns a list of objects with duplicates |\n",
    "| corr | Returns the Pearson Correlation Coefficient for two columns |\n",
    "| max | Compute the max value for each numeric columns for each group |\n",
    "| mean | Compute the average value for each numeric columns for each group |\n",
    "| stddev_samp | Returns the sample standard deviation of the expression in a group |\n",
    "| sumDistinct | Returns the sum of distinct values in the expression |\n",
    "| var_pop | Returns the population variance of the values in a group |\n",
    "\n",
    "Use the grouped data method <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.GroupedData.agg.html#pyspark.sql.GroupedData.agg\" target=\"_blank\">**`agg`**</a> to apply built-in aggregate functions\n",
    "\n",
    "This allows you to apply other transformations on the resulting columns, such as <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html\" target=\"_blank\">**`alias`**</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1336b017-6419-43f1-b1cb-9d40d689be2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "state_purchases_df = df.groupBy(\"geo.state\").agg(sum(\"ecommerce.total_item_quantity\").alias(\"total_purchases\"))\n",
    "display(state_purchases_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2c49af4-6b69-4c9d-8d13-a89d2e0690ea",
     "showTitle": false,
     "title": "--i18n-875e6ef8-fec3-4468-ab86-f3f6946b281f"
    }
   },
   "source": [
    "\n",
    "Apply multiple aggregate functions on grouped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7dced6a2-2dfb-434d-9eb6-45beb33a4f90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg, approx_count_distinct\n",
    "\n",
    "state_aggregates_df = (df\n",
    "                       .groupBy(\"geo.state\")\n",
    "                       .agg(avg(\"ecommerce.total_item_quantity\").alias(\"avg_quantity\"),\n",
    "                            approx_count_distinct(\"user_id\").alias(\"distinct_users\"))\n",
    "                      )\n",
    "\n",
    "display(state_aggregates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34de8342-b739-4e82-9ab8-58c199c427f4",
     "showTitle": false,
     "title": "--i18n-6bb4a15f-4f5d-4f70-bf50-4be00167d9fa"
    }
   },
   "source": [
    "\n",
    "### Math Functions\n",
    "Here are some of the built-in functions for math operations.\n",
    "\n",
    "| Method | Description |\n",
    "| --- | --- |\n",
    "| ceil | Computes the ceiling of the given column. |\n",
    "| cos | Computes the cosine of the given value. |\n",
    "| log | Computes the natural logarithm of the given value. |\n",
    "| round | Returns the value of the column e rounded to 0 decimal places with HALF_UP round mode. |\n",
    "| sqrt | Computes the square root of the specified float value. |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a17ad7ac-4f67-473b-9d14-35fb3388c392",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import cos, sqrt\n",
    "\n",
    "display(spark.range(10)  # Create a DataFrame with a single column called \"id\" with a range of integer values\n",
    "        .withColumn(\"sqrt\", sqrt(\"id\"))\n",
    "        .withColumn(\"cos\", cos(\"id\"))\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5784fd3c-bbbf-4eae-ba5f-de97f436e33f",
     "showTitle": false,
     "title": "--i18n-d03fb77f-5e4c-43b8-a293-884cd7cb174c"
    }
   },
   "source": [
    "\n",
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d57abdea-f146-4879-b4df-782961dff432",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff1dba27-55e6-4a06-b9eb-5e0d2325d91d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "&copy; 2023 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "DE 0.05 - Aggregation_clone",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
